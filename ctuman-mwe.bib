@article{PARISI201954,
title = {Continual lifelong learning with neural networks: A review},
journal = {Neural Networks},
volume = {113},
pages = {54-71},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2019.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0893608019300231},
author = {German I. Parisi and Ronald Kemker and Jose L. Part and Christopher Kanan and Stefan Wermter},
keywords = {Continual learning, Lifelong learning, Catastrophic forgetting, Developmental systems, Memory consolidation},
abstract = {Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for computational learning systems and autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. Although significant advances have been made in domain-specific learning with neural networks, extensive research efforts are required for the development of robust lifelong learning on autonomous agents and robots. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.}
}

@misc{oclsurvey_graphs,
      title={Online Continual Learning: A Systematic Literature Review of Approaches, Challenges, and Benchmarks}, 
      author={Seyed Amir Bidaki and Amir Mohammadkhah and Kiyan Rezaee and Faeze Hassani and Sadegh Eskandari and Maziar Salahi and Mohammad M. Ghassemi},
      year={2025},
      eprint={2501.04897},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.04897}, 
}
@misc{clsurvey,
      title={A Comprehensive Survey of Continual Learning: Theory, Method and Application}, 
      author={Liyuan Wang and Xingxing Zhang and Hang Su and Jun Zhu},
      year={2024},
      eprint={2302.00487},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2302.00487}, 
}
@incollection{VALLAR200114049,
title = {Short-term Memory: Psychological and Neural Aspects},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {14049-14055},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/03515-4},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767035154},
author = {G. Vallar},
abstract = {‘Short-term memory’ refers to a number of systems with limited capacity, which secure the temporary retention (in the range of seconds) of a variety of materials. The three more extensively investigated short-term memory systems consist of a phonological component, concerned with the retention of verbal material, a visual component, involved in recognition memory, and a spatial component, retaining information about spatial location. The general functional architecture of short-term memory includes a store and a rehearsal process, which refreshes the temporary trace, preventing its decay. These different retention systems are functionally independent, have discrete neural correlates in the association cortical areas, and may be selectively disrupted by brain damage. Phonological short-term memory is a mainly left-hemisphere-based system, including the posterior–inferior parietal (store component), and premotor (rehearsal component) regions. Memory for spatial location is a mainly right-hemisphere-based system, comprising the association occipital, posterior–inferior parietal and dorsolateral frontal cortices (‘dorsal visual stream’). Visual recognition memory is associated with a mainly left-hemisphere-based network including the association parietal, temporal, and ventral frontal cortices (‘ventral visual stream’). In addition to supporting temporary retention for a variety of purposes, short-term memory systems participate in the long-term learning of new information. One such case is the acquisition of new vocabulary in a native or second language by children and adults.}
}

@article {Sigman7585,
	author = {Sigman, Mariano and Dehaene, Stanislas},
	title = {Brain Mechanisms of Serial and Parallel Processing during Dual-Task Performance},
	volume = {28},
	number = {30},
	pages = {7585--7598},
	year = {2008},
	doi = {10.1523/JNEUROSCI.0948-08.2008},
	publisher = {Society for Neuroscience},
	abstract = {The psychological refractory period (PRP) refers to the fact that humans typically cannot perform two tasks at once. Behavioral experiments have led to the proposal that, in fact, peripheral perceptual and motor stages continue to operate in parallel, and that only a central decision stage imposes a serial bottleneck. We tested this model using neuroimaging methods combined with innovative time-sensitive analysis tools. Subjects performed a dual-task visual{\textendash}auditory paradigm in which a delay of 300 ms was injected into the auditory task either within or outside of the dual-task interference period. Event-related potentials indicated that the first \~{}250 ms of processing were insensitive to dual-task interference, and that the PRP was mainly reflected in a delayed global component. By a clustering analysis based on time-resolved functional magnetic resonance imaging, we identified networks with qualitatively different timing properties: sensory areas tracked the objective time of stimulus presentation, a bilateral parietoprefrontal network correlated with the PRP delay, and an extended bilateral network that included bilateral posterior parietal cortex, premotor cortex, supplementary motor area, anterior part of the insula, and cerebellum was shared by both tasks during the extent of dual-task performance. The results provide physiological evidence for the coexistence of serial and parallel processes within a cognitive task.},
	issn = {0270-6474},
	URL = {https://www.jneurosci.org/content/28/30/7585},
	eprint = {https://www.jneurosci.org/content/28/30/7585.full.pdf},
	journal = {Journal of Neuroscience}
}

@article{EWC,
   title={Overcoming catastrophic forgetting in neural networks},
   volume={114},
   ISSN={1091-6490},
   url={http://dx.doi.org/10.1073/pnas.1611835114},
   DOI={10.1073/pnas.1611835114},
   number={13},
   journal={Proceedings of the National Academy of Sciences},
   publisher={Proceedings of the National Academy of Sciences},
   author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
   year={2017},
   month=mar, pages={3521–3526} }

@misc{LwF,
      title={Learning without Forgetting}, 
      author={Zhizhong Li and Derek Hoiem},
      year={2017},
      eprint={1606.09282},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1606.09282}, 
}


@misc{DER,
      title={Dark Experience for General Continual Learning: a Strong, Simple Baseline}, 
      author={Pietro Buzzega and Matteo Boschini and Angelo Porrello and Davide Abati and Simone Calderara},
      year={2020},
      eprint={2004.07211},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2004.07211}, 
}

@misc{ER,
      title={On Tiny Episodic Memories in Continual Learning}, 
      author={Arslan Chaudhry and Marcus Rohrbach and Mohamed Elhoseiny and Thalaiyasingam Ajanthan and Puneet K. Dokania and Philip H. S. Torr and Marc'Aurelio Ranzato},
      year={2019},
      eprint={1902.10486},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1902.10486}, 
}

@misc{GSS,
      title={Gradient based sample selection for online continual learning}, 
      author={Rahaf Aljundi and Min Lin and Baptiste Goujaud and Yoshua Bengio},
      year={2019},
      eprint={1903.08671},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1903.08671}, 
}

@misc{MIR,
      title={Online Continual Learning with Maximally Interfered Retrieval}, 
      author={Rahaf Aljundi and Lucas Caccia and Eugene Belilovsky and Massimo Caccia and Min Lin and Laurent Charlin and Tinne Tuytelaars},
      year={2019},
      eprint={1908.04742},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1908.04742}, 
}

@misc{DGR,
      title={Continual Learning with Deep Generative Replay}, 
      author={Hanul Shin and Jung Kwon Lee and Jaehong Kim and Jiwon Kim},
      year={2017},
      eprint={1705.08690},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1705.08690}, 
}

@inproceedings{brain-self-supervised,
 author = {Schaeffer, Rylan and Khona, Mikail and Ma, Tzuhsuan and Eyzaguirre, Cristobal and Koyejo, Sanmi and Fiete, Ila},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {23140--23157},
 publisher = {Curran Associates, Inc.},
 title = {Self-Supervised Learning of Representations for Space Generates Multi-Modular Grid Cells},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/4846257e355f6923fc2a1fbe35099e91-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@inproceedings{RM,
author = {Bang, Jihwan and Kim, Heesu and Yoo, Youngjoon and Ha, Jung-Woo and Choi, Jonghyun},
year = {2021},
month = {10},
pages = {},
title = {Rainbow Memory: Continual Learning with a Memory of Diverse Samples},
doi = {10.1109/CVPR46437.2021.00812}
}

@article{AQM,
  title={Online Learned Continual Compression with Stacked Quantization Module},
  author={Lucas Caccia and Eugene Belilovsky and Massimo Caccia and Joelle Pineau},
  journal={ArXiv},
  year={2019},
  volume={abs/1911.08019},
  url={https://api.semanticscholar.org/CorpusID:208158188}
}

@article{usesOfAI,
  title = {Machine Learning: Algorithms,  Real-World Applications and Research Directions},
  volume = {2},
  ISSN = {2661-8907},
  url = {http://dx.doi.org/10.1007/s42979-021-00592-x},
  DOI = {10.1007/s42979-021-00592-x},
  number = {3},
  journal = {SN Computer Science},
  publisher = {Springer Science and Business Media LLC},
  author = {Sarker,  Iqbal H.},
  year = {2021},
  month = mar 
}

@article{
DINOv2,
title={{DINO}v2: Learning Robust Visual Features without Supervision},
author={Maxime Oquab and Timoth{\'e}e Darcet and Th{\'e}o Moutakanni and Huy V. Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel HAZIZA and Francisco Massa and Alaaeldin El-Nouby and Mido Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Herve Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=a68SUt6zFt},
note={Featured Certification}
}

@article{HAT,
      title={HAT-CL: A Hard-Attention-to-the-Task PyTorch Library for Continual Learning}, 
      author={Xiaotian Duan},
      year={2024},
      eprint={2307.09653},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.09653}, 
}

@inproceedings{ACL,
author = {Ebrahimi, Sayna and Meier, Franziska and Calandra, Roberto and Darrell, Trevor and Rohrbach, Marcus},
title = {Adversarial Continual Learning},
year = {2020},
isbn = {978-3-030-58620-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58621-8_23},
doi = {10.1007/978-3-030-58621-8_23},
abstract = {Continual learning aims to learn new tasks without forgetting previously learned ones. We hypothesize that representations learned to solve each task in a sequence have a shared structure while containing some task-specific properties. We show that shared features are significantly less prone to forgetting and propose a novel hybrid continual learning framework that learns a disjoint representation for task-invariant and task-specific features required to solve a sequence of tasks. Our model combines architecture growth to prevent forgetting of task-specific skills and an experience replay approach to preserve shared skills. We demonstrate our hybrid approach is effective in avoiding forgetting and show it is superior to both architecture-based and memory-based approaches on class incrementally learning of a single dataset as well as a sequence of multiple datasets in image classification. Our code is available at .},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XI},
pages = {386–402},
numpages = {17},
location = {Glasgow, United Kingdom}
}

@INPROCEEDINGS{GFR,
  author={Liu, Xialei and Wu, Chenshen and Menta, Mikel and Herranz, Luis and Raducanu, Bogdan and Bagdanov, Andrew D. and Jui, Shangling and van de Weijer, Joost},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
  title={Generative Feature Replay For Class-Incremental Learning}, 
  year={2020},
  volume={},
  number={},
  pages={915-924},
  keywords={Task analysis;Feature extraction;Gallium nitride;Image generation;Correlation;Training;Generators},
  doi={10.1109/CVPRW50498.2020.00121}}

@Inbook{SGD,
author="Bottou, L{\'e}on",
editor="Bousquet, Olivier
and von Luxburg, Ulrike
and R{\"a}tsch, Gunnar",
title="Stochastic Learning",
bookTitle="Advanced Lectures on Machine Learning: ML Summer Schools 2003, Canberra, Australia, February 2 - 14, 2003, T{\"u}bingen, Germany, August 4 - 16, 2003, Revised Lectures",
year="2004",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="146--168",
abstract="This contribution presents an overview of the theoretical and practical aspects of the broad family of learning algorithms based on Stochastic Gradient Descent, including Perceptrons, Adalines, K-Means, LVQ, Multi-Layer Networks, and Graph Transformer Networks.",
isbn="978-3-540-28650-9",
doi="10.1007/978-3-540-28650-9_7",
url="https://doi.org/10.1007/978-3-540-28650-9_7"
}

@article{self-supervized-generalization,
title = {A study of the generalizability of self-supervised representations},
journal = {Machine Learning with Applications},
volume = {6},
pages = {100124},
year = {2021},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2021.100124},
url = {https://www.sciencedirect.com/science/article/pii/S2666827021000621},
author = {Atharva Tendle and Mohammad Rashedul Hasan},
keywords = {Self-supervised learning, Supervised learning, Transfer learning, Generalizability, Invariance},
abstract = {Recent advancements in self-supervised learning (SSL) made it possible to learn generalizable visual representations from unlabeled data. The performance of Deep Learning models fine-tuned on pretrained SSL representations is on par with models fine-tuned on the state-of-the-art supervised learning (SL) representations. Irrespective of the progress made in SSL, its generalizability has not been studied extensively. In this article, we perform a deeper analysis of the generalizability of pretrained SSL and SL representations by conducting a domain-based study for transfer learning classification tasks. The representations are learned from the ImageNet source data, which are then fine-tuned using two types of target datasets: similar to the source dataset, and significantly different from the source dataset. We study generalizability of the SSL and SL-based models via their prediction accuracy as well as prediction confidence. In addition to this, we analyze the attribution of the final convolutional layer of these models to understand how they reason about the semantic identity of the data. We show that the SSL representations are more generalizable as compared to the SL representations. We explain the generalizability of the SSL representations by investigating its invariance property, which is shown to be better than that observed in the SL representations.}
}